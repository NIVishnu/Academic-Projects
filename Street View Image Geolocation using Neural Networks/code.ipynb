{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100013 files belonging to 50 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset=tf.keras.preprocessing.image_dataset_from_directory(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def sample_dataset(dataset_path, sample_size_per_class):\n",
    "  \"\"\"Samples the dataset with a specified number of images per class.\n",
    "\n",
    "  Args:\n",
    "    dataset_path: Path to the dataset directory.\n",
    "    sample_size_per_class: Number of images to sample per class.\n",
    "\n",
    "  Returns:\n",
    "    A list of image paths for the sampled dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  image_paths = []\n",
    "  labels = []\n",
    "\n",
    "  for class_dir in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_dir)\n",
    "    image_files = os.listdir(class_path)\n",
    "    sampled_images = np.random.choice(image_files, sample_size_per_class, replace=False)\n",
    "    for image_file in sampled_images:\n",
    "      image_path = os.path.join(class_path, image_file)\n",
    "      image_paths.append(image_path)\n",
    "      labels.append(class_dir)\n",
    "\n",
    "  return image_paths, labels\n",
    "\n",
    "# Example usage:\n",
    "dataset_path = \"test_data\"\n",
    "sample_size = 100\n",
    "sampled_image_paths, sampled_labels = sample_dataset(dataset_path, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_sample_dataset(source_dir, target_dir, sample_size):\n",
    "  \"\"\"Creates a sample dataset from the source directory.\n",
    "\n",
    "  Args:\n",
    "    source_dir: Path to the source directory.\n",
    "    target_dir: Path to the target directory.\n",
    "    sample_size: Number of images to sample per state.\n",
    "  \"\"\"\n",
    "\n",
    "  if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "  for state in os.listdir(source_dir):\n",
    "    state_dir = os.path.join(source_dir, state)\n",
    "    target_state_dir = os.path.join(target_dir, state)\n",
    "    os.makedirs(target_state_dir, exist_ok=True)\n",
    "\n",
    "    image_files = os.listdir(state_dir)\n",
    "    sampled_images = random.sample(image_files, min(sample_size, len(image_files)))\n",
    "\n",
    "    for image_file in sampled_images:\n",
    "      src_path = os.path.join(state_dir, image_file)\n",
    "      dst_path = os.path.join(target_state_dir, image_file)\n",
    "      shutil.copy(src_path, dst_path)\n",
    "\n",
    "# Example usage:\n",
    "source_dir = \"test_data\"\n",
    "target_dir = \"sampled_data\"\n",
    "sample_size = 100  # Adjust sample size as needed\n",
    "create_sample_dataset(source_dir, target_dir, sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4993 files belonging to 50 classes.\n"
     ]
    }
   ],
   "source": [
    "sampled_dataset=tf.keras.preprocessing.image_dataset_from_directory(\"sampled_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sampled_size = len(list(sampled_dataset))\n",
    "train_size = int(0.8 * total_sampled_size)\n",
    "test_size = total_sampled_size - train_size\n",
    "\n",
    "train_dataset = sampled_dataset.take(train_size)\n",
    "test_dataset = sampled_dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4993 files belonging to 50 classes.\n",
      "Training dataset: <_SkipDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "Test dataset: <_TakeDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "Validation dataset: <_TakeDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "\n",
      "Dataset Info:\n",
      "num_classes: 50\n",
      "class_names: ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
      "total_samples: 4993\n",
      "train_samples: 3995\n",
      "test_samples: 499\n",
      "val_samples: 499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 64\n",
    "224\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"sampled_data\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Get the total number of samples\n",
    "total_samples = len(full_dataset.file_paths)\n",
    "\n",
    "# Calculate split sizes\n",
    "val_size = int(0.1 * total_samples)\n",
    "test_size = int(0.1 * total_samples)\n",
    "train_size = total_samples - val_size - test_size\n",
    "\n",
    "# Create the splits\n",
    "ds_val = full_dataset.take(val_size)\n",
    "remaining = full_dataset.skip(val_size)\n",
    "ds_tst = remaining.take(test_size)\n",
    "ds_trn = remaining.skip(test_size)\n",
    "\n",
    "# Print information about the datasets\n",
    "print(\"Training dataset:\", ds_trn)\n",
    "print(\"Test dataset:\", ds_tst)\n",
    "print(\"Validation dataset:\", ds_val)\n",
    "\n",
    "# If you need dataset info\n",
    "ds_info = {\n",
    "    \"num_classes\": len(full_dataset.class_names),\n",
    "    \"class_names\": full_dataset.class_names,\n",
    "    \"total_samples\": total_samples,\n",
    "    \"train_samples\": train_size,\n",
    "    \"test_samples\": test_size,\n",
    "    \"val_samples\": val_size\n",
    "}\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "for key, value in ds_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
